{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xmltodict\n",
    "import requests as req\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML TO DIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse xml to dic\n",
    "def getxml():\n",
    "    url = 'http://tass.sepln.org/tass_data/dataset/TASS2019_country_MX_train.xml'\n",
    "    data = req.get(url).content\n",
    "    return xmltodict.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxml1():\n",
    "    url = 'http://tass.sepln.org/tass_data/dataset/TASS2019_country_MX_dev.xml'\n",
    "    data = req.get(url).content\n",
    "    return xmltodict.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxml2():\n",
    "    url = 'http://tass.sepln.org/tass_data/dataset/general-train-tagged-3l.xml'\n",
    "    data = req.get(url).content\n",
    "    return xmltodict.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict = getxml()\n",
    "\n",
    "orderedDictList = OrderedDict['tweets']['tweet']\n",
    "\n",
    "#dic to dataframe\n",
    "df = pd.DataFrame(orderedDictList)\n",
    "\n",
    "#flatten sentiment column\n",
    "df = pd.concat([df.sentiment.apply(pd.Series), df.drop('sentiment', axis=1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict1 = getxml1()\n",
    "\n",
    "orderedDictList1 = OrderedDict1['tweets']['tweet']\n",
    "\n",
    "#dic to dataframe\n",
    "df1 = pd.DataFrame(orderedDictList1)\n",
    "\n",
    "#flatten sentiment column\n",
    "df1 = pd.concat([df1.sentiment.apply(pd.Series), df1.drop('sentiment', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict2 = getxml2()\n",
    "\n",
    "orderedDictList2 = OrderedDict2['tweets']['tweet']\n",
    "\n",
    "#dic to dataframe\n",
    "df2 = pd.DataFrame(orderedDictList2)\n",
    "\n",
    "#flatten sentiment column\n",
    "df2 = pd.concat([df2.sentiments.apply(pd.Series), df2.drop('sentiments', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.append(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(s):\n",
    "    return s.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'].apply(lambda x: get_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code values to -1, 0, 1\n",
    "def code(s):\n",
    "    if 'NEU' in s:\n",
    "        s = .5\n",
    "    elif 'N' in s:\n",
    "        s = 0\n",
    "    elif 'P' in s:\n",
    "        s = 1\n",
    "    elif 'NONE' in s:\n",
    "        s = 'unknown'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['polarity'].apply(lambda x: code(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('corpus_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop tweets that don't have a polarity\n",
    "df = df[df.polarity != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only useful columns for training\n",
    "train_twitter = df[['content','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter = train_twitter.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_(s):\n",
    "    lst=[]\n",
    "    s=nlp(s)\n",
    "    for token in s:\n",
    "        lst.append(token.lemma_)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [nivel, de, ingle,    , alto,    , traducir, j...\n",
       "1       [si, ser, de, area,   , y, con, suerte, pasar,...\n",
       "2       [sabian, que, su, after, fav, teatro, gramo,  ...\n",
       "3       [y, hoy, por, primero, vez, me, sali, con, el,...\n",
       "4       [parir, que, hacer, coraje, con, ambriz,   , a...\n",
       "                              ...                        \n",
       "1353               [jancarlobg, a, vez, me, sentir, solo]\n",
       "1354    [tomar, en, miercoles,     , na,  , solo, olvi...\n",
       "1355                 [querer, y, necesitar, verte, yaaaa]\n",
       "1356    [gracia, a, comedy, central, y, mtv, ir, a, es...\n",
       "1357    [por, si, no, saber,  , al, tirar, ese, a, lo,...\n",
       "Name: content, Length: 1358, dtype: object"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_twitter['content'].apply(lambda x : lemmatize_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove links, numbers and any other simbols. Convert to lower case\n",
    "def clean_up(s):\n",
    "    s=re.sub(r'http\\S+','',s)\n",
    "    s=re.sub('\\W', ' ',s)\n",
    "    s=re.sub('\\d', ' ', s)\n",
    "    s=s.lower().strip()\n",
    "    s=s.replace('á','a')\n",
    "    s=s.replace('é','e')\n",
    "    s=s.replace('í','i')\n",
    "    s=s.replace('ó','o')\n",
    "    s=s.replace('ú','u')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter['content']=train_twitter['content'].apply(lambda x : clean_up(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    757\n",
       "1.0    471\n",
       "0.5    130\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_twitter['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('es_core_news_md')\n",
    "#parser=Spanish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_twitter['tokens']=train_twitter['content'].apply(lambda x : tokenize_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_spaces(s):\n",
    "    return [i for i in s if i != '   ' and i != '  ' and i != ' ' and i != '    ' and i != '     ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter['lemmatized']=train_twitter['content'].apply(lambda x : lemmatize_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba=train_twitter['lemmatized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter['tokens']=train_twitter['tokens'].apply(lambda x: clear_spaces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twitter['lemmatized']=train_twitter['lemmatized'].apply(lambda x: clear_spaces(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_sw(s):\n",
    "#    return [i for i in s if i not in STOP_WORDS and len(i)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_twitter['tokens']=train_twitter['tokens'].apply(lambda x: remove_sw(x))\n",
    "#train_twitter['lemmatized']=train_twitter['lemmatized'].apply(lambda x: remove_sw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wc=WordCloud().generate(','.join(train_twitter['tokens'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TdIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words = STOP_WORDS).fit(train_twitter.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vect.transform(train_twitter.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized = pd.DataFrame(x.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized=pd.concat([train_vectorized, train_twitter['polarity']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized['polarity']=train_vectorized['polarity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__walter__</th>\n",
       "      <th>_alexkun</th>\n",
       "      <th>_alineepa</th>\n",
       "      <th>_carlosrivera</th>\n",
       "      <th>_esme</th>\n",
       "      <th>_nosoyartista</th>\n",
       "      <th>_zylx</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaajajajajajajajajaj</th>\n",
       "      <th>aaah</th>\n",
       "      <th>...</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>zahirecoronel</th>\n",
       "      <th>zavala</th>\n",
       "      <th>ziempre</th>\n",
       "      <th>zommix</th>\n",
       "      <th>zona</th>\n",
       "      <th>zone</th>\n",
       "      <th>ñammm</th>\n",
       "      <th>ñoña</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1358 rows × 4887 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      __walter__  _alexkun  _alineepa  _carlosrivera  _esme  _nosoyartista  \\\n",
       "0            0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "1            0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "2            0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "3            0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "4            0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "...          ...       ...        ...            ...    ...            ...   \n",
       "1353         0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "1354         0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "1355         0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "1356         0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "1357         0.0       0.0        0.0            0.0    0.0            0.0   \n",
       "\n",
       "      _zylx  aaaa  aaaaajajajajajajajajaj  aaah  ...  yyyy  zahirecoronel  \\\n",
       "0       0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "1       0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "2       0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "3       0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "4       0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "...     ...   ...                     ...   ...  ...   ...            ...   \n",
       "1353    0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "1354    0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "1355    0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "1356    0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "1357    0.0   0.0                     0.0   0.0  ...   0.0            0.0   \n",
       "\n",
       "      zavala  ziempre  zommix  zona  zone  ñammm  ñoña  polarity  \n",
       "0        0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "1        0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "2        0.0      0.0     0.0   0.0   0.0    0.0   0.0         1  \n",
       "3        0.0      0.0     0.0   0.0   0.0    0.0   0.0         1  \n",
       "4        0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "...      ...      ...     ...   ...   ...    ...   ...       ...  \n",
       "1353     0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "1354     0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "1355     0.0      0.0     0.0   0.0   0.0    0.0   0.0         1  \n",
       "1356     0.0      0.0     0.0   0.0   0.0    0.0   0.0         1  \n",
       "1357     0.0      0.0     0.0   0.0   0.0    0.0   0.0         0  \n",
       "\n",
       "[1358 rows x 4887 columns]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB, ComplementNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7095588235294118\n",
      "[[0.66911765 0.00367647]\n",
      " [0.28676471 0.04044118]]\n"
     ]
    }
   ],
   "source": [
    "# Define X and y\n",
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "# Train\n",
    "log_reg = LogisticRegression().fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print accuracy score and confusion matrix on test set\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_predicted))\n",
    "print(confusion_matrix(y_test,y_predicted)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAUSSIAN NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.6213235294117647\n",
      "[[0.42279412 0.25      ]\n",
      " [0.12867647 0.19852941]]\n"
     ]
    }
   ],
   "source": [
    "# Define X and y\n",
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTINOMIAL NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7279411764705882\n",
      "[[0.65441176 0.01838235]\n",
      " [0.25367647 0.07352941]]\n"
     ]
    }
   ],
   "source": [
    "# Define X and y\n",
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.6838235294117647\n",
      "[[0.65808824 0.01470588]\n",
      " [0.30147059 0.02573529]]\n"
     ]
    }
   ],
   "source": [
    "# Define X and y\n",
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "bnb=BernoulliNB()\n",
    "\n",
    "y_pred = bnb.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7169117647058824\n",
      "[[0.66911765 0.00367647]\n",
      " [0.27941176 0.04779412]]\n"
     ]
    }
   ],
   "source": [
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "svc=SVC()\n",
    "\n",
    "y_pred = svc.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.6727941176470589\n",
      "[[0.4375     0.23529412]\n",
      " [0.09191176 0.23529412]]\n"
     ]
    }
   ],
   "source": [
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "cnb=ComplementNB()\n",
    "\n",
    "y_pred = cnb.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7279411764705882\n",
      "[[0.63970588 0.03308824]\n",
      " [0.23897059 0.08823529]]\n"
     ]
    }
   ],
   "source": [
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "gbc=GBC()\n",
    "\n",
    "y_pred = gbc.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as XGBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7242647058823529\n",
      "[[0.60661765 0.06617647]\n",
      " [0.20955882 0.11764706]]\n"
     ]
    }
   ],
   "source": [
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "xgbc=XGBC()\n",
    "\n",
    "y_pred = xgbc.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:  0.7316176470588235\n",
      "[[0.63970588 0.03308824]\n",
      " [0.23529412 0.09191176]]\n"
     ]
    }
   ],
   "source": [
    "y = train_vectorized.polarity\n",
    "X = train_vectorized.drop('polarity', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=46)\n",
    "\n",
    "rfc=RFC()\n",
    "\n",
    "y_pred = rfc.fit(X_train, y_train).predict(X_test)\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"Pickle_RFC_Model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(rfc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_RFC_Model = pickle.load(file)\n",
    "\n",
    "Pickled_RFC_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = Pickled_RFC_Model.predict(XX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.7+8-LTS, mixed mode)\n",
      "  Starting server from C:\\Users\\danie\\anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\tmpfeujc7so\n",
      "  JVM stdout: C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\tmpfeujc7so\\h2o_danie_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\tmpfeujc7so\\h2o_danie_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>06 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Mexico_City</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.30.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>17 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_danie_ocslln</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>1.971 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         06 secs\n",
       "H2O_cluster_timezone:       America/Mexico_City\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.30.0.2\n",
       "H2O_cluster_version_age:    17 days\n",
       "H2O_cluster_name:           H2O_from_python_danie_ocslln\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    1.971 Gb\n",
       "H2O_cluster_total_cores:    0\n",
       "H2O_cluster_allowed_cores:  0\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.6 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized.to_csv('trainh20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train=h2o.import_file('trainh20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.columns\n",
    "y='polarity'\n",
    "X.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |\n",
      "17:19:21.238: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "aml=H2OAutoML(max_models=10, seed=1)\n",
    "aml.train(x=X, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=aml.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">  mean_residual_deviance</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">   rmsle</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20200516_171921</td><td style=\"text-align: right;\">                0.175016</td><td style=\"text-align: right;\">0.418349</td><td style=\"text-align: right;\">0.175016</td><td style=\"text-align: right;\">0.35537 </td><td style=\"text-align: right;\">0.29333 </td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20200516_171921   </td><td style=\"text-align: right;\">                0.175211</td><td style=\"text-align: right;\">0.418583</td><td style=\"text-align: right;\">0.175211</td><td style=\"text-align: right;\">0.356024</td><td style=\"text-align: right;\">0.293454</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.184019</td><td style=\"text-align: right;\">0.428975</td><td style=\"text-align: right;\">0.184019</td><td style=\"text-align: right;\">0.375741</td><td style=\"text-align: right;\">0.300958</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.184747</td><td style=\"text-align: right;\">0.429822</td><td style=\"text-align: right;\">0.184747</td><td style=\"text-align: right;\">0.371523</td><td style=\"text-align: right;\">0.301468</td></tr>\n",
       "<tr><td>GBM_3_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.185442</td><td style=\"text-align: right;\">0.43063 </td><td style=\"text-align: right;\">0.185442</td><td style=\"text-align: right;\">0.375282</td><td style=\"text-align: right;\">0.301968</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.187915</td><td style=\"text-align: right;\">0.433492</td><td style=\"text-align: right;\">0.187915</td><td style=\"text-align: right;\">0.383834</td><td style=\"text-align: right;\">0.304471</td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20200516_171921_model_1         </td><td style=\"text-align: right;\">                0.188263</td><td style=\"text-align: right;\">0.433893</td><td style=\"text-align: right;\">0.188263</td><td style=\"text-align: right;\">0.390407</td><td style=\"text-align: right;\">0.304303</td></tr>\n",
       "<tr><td>XRT_1_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.189461</td><td style=\"text-align: right;\">0.435272</td><td style=\"text-align: right;\">0.189461</td><td style=\"text-align: right;\">0.357056</td><td style=\"text-align: right;\">0.303179</td></tr>\n",
       "<tr><td>DRF_1_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.190027</td><td style=\"text-align: right;\">0.435921</td><td style=\"text-align: right;\">0.190027</td><td style=\"text-align: right;\">0.357961</td><td style=\"text-align: right;\">0.303662</td></tr>\n",
       "<tr><td>GLM_1_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.210235</td><td style=\"text-align: right;\">0.458514</td><td style=\"text-align: right;\">0.210235</td><td style=\"text-align: right;\">0.433346</td><td style=\"text-align: right;\">0.321974</td></tr>\n",
       "<tr><td>GBM_5_AutoML_20200516_171921                       </td><td style=\"text-align: right;\">                0.226237</td><td style=\"text-align: right;\">0.475643</td><td style=\"text-align: right;\">0.226237</td><td style=\"text-align: right;\">0.449944</td><td style=\"text-align: right;\">0.334524</td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20200516_171921              </td><td style=\"text-align: right;\">                0.238038</td><td style=\"text-align: right;\">0.487891</td><td style=\"text-align: right;\">0.238038</td><td style=\"text-align: right;\">0.410113</td><td style=\"text-align: right;\">0.336574</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.head(rows=lb.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('testh20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "test=h2o.import_file('testh20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
